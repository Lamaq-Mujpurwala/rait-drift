"""
LLM response generation — assembles prompt, calls Groq via litellm,
post-processes with mandatory disclaimers.
"""

from __future__ import annotations

import time
from dataclasses import dataclass
from typing import Optional

from src.utils.config import GROQ_API_KEY, LLMConfig, load_config
from src.utils.rate_limiter import rate_limited_completion
from src.production.retrieval import RetrievalResult


# ── Data Classes ─────────────────────────────────────────────────────────────

@dataclass
class GenerationResult:
    """Result of LLM generation."""
    raw_response: str
    final_response: str
    model_name: str
    temperature: float
    prompt_tokens: int
    completion_tokens: int
    finish_reason: str
    latency_ms: int


# ── Constants ────────────────────────────────────────────────────────────────

SYSTEM_PROMPT = """You are a UK government services assistant. You help citizens \
understand housing, benefits, and local council services based ONLY on the \
official GOV.UK information provided below.

RULES:
1. Only answer based on the provided context. If the context doesn't contain \
the answer, say "I don't have enough information to answer that. Please \
visit GOV.UK or contact your local council."
2. Always cite which GOV.UK page your answer comes from.
3. Include relevant eligibility criteria, thresholds, and deadlines.
4. Use plain English (reading age 9, per GOV.UK style guide).
5. If the question involves financial thresholds or dates, state them explicitly.
6. Never provide personal financial or legal advice."""

DISCLAIMER = (
    "\n\n---\n*This response is generated by an AI assistant using official "
    "GOV.UK information. It is not personal advice. For definitive guidance, "
    "visit [GOV.UK](https://www.gov.uk) or contact your local council. "
    "Information may have changed since the source was last updated.*"
)


# ── Response Generator ───────────────────────────────────────────────────────

class ResponseGenerator:
    """Assembles prompt, calls Groq LLM via litellm, post-processes response."""

    def __init__(self, config: Optional[LLMConfig] = None):
        self.config = config or load_config().primary_llm
        # Set Groq API key for litellm
        import os
        os.environ["GROQ_API_KEY"] = GROQ_API_KEY

    def generate(self, query: str, retrieval: RetrievalResult) -> GenerationResult:
        """Generate a response using the LLM with retrieved context."""

        user_prompt = f"""Context from GOV.UK (official UK government information):

{retrieval.context}

---

Citizen's question: {query}

Please provide a helpful, accurate answer based only on the context above."""

        start_time = time.perf_counter()

        response = rate_limited_completion(
            model=self.config.model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
        )

        elapsed_ms = int((time.perf_counter() - start_time) * 1000)

        raw_response = response.choices[0].message.content or ""
        final_response = raw_response + DISCLAIMER

        return GenerationResult(
            raw_response=raw_response,
            final_response=final_response,
            model_name=self.config.model,
            temperature=self.config.temperature,
            prompt_tokens=response.usage.prompt_tokens if response.usage else 0,
            completion_tokens=response.usage.completion_tokens if response.usage else 0,
            finish_reason=response.choices[0].finish_reason or "",
            latency_ms=elapsed_ms,
        )
