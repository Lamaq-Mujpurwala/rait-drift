"""
╔══════════════════════════════════════════════════════════════════════════════╗
║  CDS — Composite Drift Signal · Detailed Test Suite                        ║
╚══════════════════════════════════════════════════════════════════════════════╝

PURPOSE
-------
CDS is the **primary passive drift detector**. It monitors distribution shifts
across K=9 text descriptors extracted from every production log entry. Unlike
TRCI (active probing), CDS is entirely passive — it only analyses data that
already flows through the system.

MATHEMATICAL FOUNDATION
-----------------------
Given K descriptors, each with a weight wₖ and JSD divergence dₖ:

    CDS = Σₖ (wₖ · dₖ) / Σₖ wₖ     (weighted average of per-descriptor JSDs)

where dₖ = JSD(P_ref_k || P_cur_k) and:

    JSD(P || Q) = ½ KL(P || M) + ½ KL(Q || M),   M = ½(P + Q)

Properties of JSD:
    - Symmetric: JSD(P || Q) = JSD(Q || P)
    - Bounded: 0 ≤ JSD ≤ ln(2) ≈ 0.693
    - Zero iff P = Q
    - Defined even when supports differ (unlike KL divergence)

The 9 descriptors and their weights:

    Descriptor               Weight  Type        Intuition
    ─────────────────────────────────────────────────────────────────
    response_length          0.15    continuous   Are responses getting shorter/longer?
    response_sentiment       0.10    continuous   Is tone drifting positive/negative?
    response_readability     0.10    continuous   Are responses becoming harder to read?
    citation_count           0.15    discrete     Are citations disappearing?
    hedge_word_count         0.10    discrete     More hedging = less confident model?
    refusal_flag             0.15    binary       Is the refusal rate changing?
    response_latency_ms      0.10    continuous   Infrastructure/model latency shifts?
    mean_retrieval_distance  0.10    continuous   Is retrieval quality changing?
    context_token_ratio      0.05    continuous   Are we sending more/less context?

Persistence filter:
    RED status only fires if CDS ≥ amber_threshold for P=3 consecutive windows.
    This prevents single-window noise from triggering false alarms.

INTUITION
---------
CDS is like a "health check panel" — each descriptor is a vital sign.
No single vital sign tells the whole story, but a composite score that
combines them (weighted by importance) gives a reliable overall reading.
The persistence filter acts like "confirming a diagnosis" — you don't
treat a condition based on a single test; you wait for it to reproduce.

WHAT THE TESTS VALIDATE
------------------------
1. JSD mathematical properties (symmetry, boundedness, identity of indiscernibles)
2. Per-descriptor type handling (continuous vs discrete vs binary)
3. Weight normalisation
4. Persistence filter logic
5. Edge cases (empty data, single value, identical distributions)
6. Threshold classification correctness

PLOTS GENERATED (via plotting tests)
-------------------------------------
1. JSD vs Distribution Shift: shows how JSD increases as we shift a normal
   distribution by increasing amounts. Validates monotonicity.
2. Per-Descriptor Contribution Bar Chart: stacked bar showing which
   descriptors contribute most to drift.
3. Persistence Timeline: shows CDS over consecutive windows with
   persistence threshold line.

EXTERNAL DATA SOURCES
---------------------
- None directly. CDS operates on production logs generated by our system.
- The reference and current distributions are synthetic in tests.
- In production, they come from the SQLite production_logs table.
"""

import numpy as np
import pytest

from src.monitoring.descriptors import DescriptorSet
from src.monitoring.metrics.cds import CDSEngine, DescriptorDriftResult, DEFAULT_DESCRIPTORS
from src.monitoring.metrics.base import MetricResult
from src.utils.config import CDSConfig, CDSDescriptorConfig
from src.utils.statistics import continuous_jsd, discrete_jsd, binary_jsd


# ═══════════════════════════════════════════════════════════════════════════════
# FIXTURES
# ═══════════════════════════════════════════════════════════════════════════════

@pytest.fixture
def stable_ref():
    """Reference distribution: stable, well-behaved production logs."""
    np.random.seed(42)
    return DescriptorSet(
        data={
            "response_length": np.random.normal(150, 20, 200),
            "response_sentiment": np.random.normal(0.3, 0.1, 200),
            "response_readability": np.random.normal(65, 10, 200),
            "citation_count": np.random.choice([1, 2, 3], 200).astype(float),
            "hedge_word_count": np.random.choice([0, 1, 2], 200).astype(float),
            "refusal_flag": np.random.choice([0, 1], 200, p=[0.9, 0.1]).astype(float),
            "response_latency_ms": np.random.normal(1000, 200, 200),
            "mean_retrieval_distance": np.random.normal(0.15, 0.05, 200),
            "context_token_ratio": np.random.normal(2.5, 0.5, 200),
        },
        query_ids=[f"ref_{i}" for i in range(200)],
    )


@pytest.fixture
def stable_cur():
    """Current distribution: drawn from SAME parameters as reference."""
    np.random.seed(123)
    return DescriptorSet(
        data={
            "response_length": np.random.normal(150, 20, 200),
            "response_sentiment": np.random.normal(0.3, 0.1, 200),
            "response_readability": np.random.normal(65, 10, 200),
            "citation_count": np.random.choice([1, 2, 3], 200).astype(float),
            "hedge_word_count": np.random.choice([0, 1, 2], 200).astype(float),
            "refusal_flag": np.random.choice([0, 1], 200, p=[0.9, 0.1]).astype(float),
            "response_latency_ms": np.random.normal(1000, 200, 200),
            "mean_retrieval_distance": np.random.normal(0.15, 0.05, 200),
            "context_token_ratio": np.random.normal(2.5, 0.5, 200),
        },
        query_ids=[f"cur_{i}" for i in range(200)],
    )


@pytest.fixture
def drifted_cur():
    """Current distribution with significant drift across multiple descriptors."""
    np.random.seed(77)
    return DescriptorSet(
        data={
            "response_length": np.random.normal(80, 30, 200),       # shorter responses
            "response_sentiment": np.random.normal(-0.1, 0.2, 200), # more negative
            "response_readability": np.random.normal(45, 15, 200),   # harder to read
            "citation_count": np.random.choice([0, 1], 200).astype(float),  # fewer citations
            "hedge_word_count": np.random.choice([2, 3, 4], 200).astype(float),  # more hedging
            "refusal_flag": np.random.choice([0, 1], 200, p=[0.5, 0.5]).astype(float),  # 50% refusals
            "response_latency_ms": np.random.normal(2000, 500, 200),    # slower
            "mean_retrieval_distance": np.random.normal(0.35, 0.1, 200), # worse retrieval
            "context_token_ratio": np.random.normal(1.0, 0.3, 200),     # less context
        },
        query_ids=[f"drift_{i}" for i in range(200)],
    )


# ═══════════════════════════════════════════════════════════════════════════════
# GROUP 1: JSD MATHEMATICAL PROPERTIES
# These tests validate the statistical foundation that CDS relies on.
# ═══════════════════════════════════════════════════════════════════════════════

class TestJSDProperties:
    """
    JSD must satisfy specific mathematical properties for CDS to be valid.
    If any of these fail, CDS's drift detection is mathematically unsound.
    """

    def test_jsd_identity_of_indiscernibles(self):
        """
        JSD(P, P) = 0    (identical distributions have zero divergence)

        Math: If P = Q, then M = P = Q, so KL(P||M) = KL(Q||M) = 0.
        """
        np.random.seed(42)
        x = np.random.normal(0, 1, 1000)
        assert continuous_jsd(x, x) == pytest.approx(0.0, abs=1e-6)

    def test_jsd_symmetry(self):
        """
        JSD(P, Q) = JSD(Q, P)    (symmetry — unlike KL divergence)

        This is why JSD was chosen over KL: the operator shouldn't need
        to know which distribution is "reference" and which is "current"
        for the divergence to be self-consistent.
        """
        np.random.seed(42)
        p = np.random.normal(0, 1, 1000)
        q = np.random.normal(2, 1, 1000)
        assert continuous_jsd(p, q) == pytest.approx(continuous_jsd(q, p), abs=1e-6)

    def test_jsd_bounded(self):
        """
        0 ≤ JSD(P, Q) ≤ ln(2) ≈ 0.693

        This boundedness is crucial — it means CDS values are interpretable
        on a fixed scale regardless of the descriptor.
        """
        np.random.seed(42)
        p = np.random.normal(0, 1, 1000)
        q = np.random.normal(10, 1, 1000)  # very different
        jsd = continuous_jsd(p, q)
        assert 0 <= jsd <= np.log(2) + 0.01, f"JSD={jsd} outside [0, ln(2)]"

    def test_jsd_monotonic_with_shift(self):
        """
        JSD increases monotonically as we shift the distribution further.

        Math: For P ~ N(0,1) and Q ~ N(μ,1), JSD(P,Q) increases with |μ|.
        This is the fundamental reason JSD works as a drift detector.
        """
        np.random.seed(42)
        p = np.random.normal(0, 1, 2000)
        shifts = [0.5, 1.0, 2.0, 4.0]
        jsds = []
        for s in shifts:
            q = np.random.normal(s, 1, 2000)
            jsds.append(continuous_jsd(p, q, bins=50))

        # Each subsequent JSD should be larger
        for i in range(len(jsds) - 1):
            assert jsds[i] < jsds[i + 1], (
                f"JSD not monotonic: shift={shifts[i]} → {jsds[i]:.4f}, "
                f"shift={shifts[i+1]} → {jsds[i+1]:.4f}"
            )

    def test_discrete_jsd_identical(self):
        """Discrete JSD of identical categorical distributions = 0."""
        x = np.array([1, 2, 3, 1, 2, 3, 1, 2, 3])
        assert discrete_jsd(x, x) == pytest.approx(0.0, abs=1e-6)

    def test_discrete_jsd_different(self):
        """Discrete JSD of different categoricals > 0."""
        x = np.array([1, 1, 1, 2, 2, 2])
        y = np.array([3, 3, 3, 4, 4, 4])
        assert discrete_jsd(x, y) > 0

    def test_binary_jsd_identical(self):
        """Binary JSD of same proportion = 0."""
        assert binary_jsd(0.5, 0.5) == pytest.approx(0.0, abs=1e-6)
        assert binary_jsd(0.1, 0.1) == pytest.approx(0.0, abs=1e-6)

    def test_binary_jsd_extreme_shift(self):
        """Binary JSD: 10% refusal → 90% refusal is a massive shift."""
        jsd = binary_jsd(0.1, 0.9)
        assert jsd > 0.3, f"Expected large JSD, got {jsd}"

    def test_binary_jsd_symmetry(self):
        """Binary JSD is symmetric."""
        assert binary_jsd(0.2, 0.8) == pytest.approx(binary_jsd(0.8, 0.2), abs=1e-10)


# ═══════════════════════════════════════════════════════════════════════════════
# GROUP 2: CDS COMPUTATION
# Tests the main compute() pipeline with controlled synthetic data.
# ═══════════════════════════════════════════════════════════════════════════════

class TestCDSComputation:
    """
    End-to-end CDS computation tests.
    """

    def test_identical_distributions_near_zero(self, stable_ref, stable_cur):
        """
        When ref and current are drawn from the same distribution,
        CDS should be very small (near 0) due to sampling noise.
        """
        cds = CDSEngine()
        result = cds.compute(stable_ref, stable_cur)
        assert result.value < 0.05, f"CDS of same-distribution = {result.value}"
        assert result.status == "GREEN"

    def test_drifted_distributions_elevated(self, stable_ref, drifted_cur):
        """
        When current distribution has drifted significantly,
        CDS should be elevated (>0.05).
        """
        cds = CDSEngine()
        result = cds.compute(stable_ref, drifted_cur)
        assert result.value > 0.05, f"CDS of drifted data = {result.value}"
        assert result.status in ("AMBER", "RED")

    def test_cds_is_weighted_average(self, stable_ref, drifted_cur):
        """
        Verify CDS = Σ(wₖ·dₖ) / Σwₖ by manual computation.

        This directly validates the mathematical formula.
        """
        cds = CDSEngine()
        result = cds.compute(stable_ref, drifted_cur)

        # Recompute manually from per_descriptor details
        per_desc = result.details["per_descriptor"]
        manual_weighted_sum = 0.0
        manual_total_weight = 0.0
        for name, info in per_desc.items():
            manual_weighted_sum += info["jsd"] * info["weight"]
            manual_total_weight += info["weight"]

        manual_cds = manual_weighted_sum / manual_total_weight if manual_total_weight > 0 else 0
        assert result.value == pytest.approx(manual_cds, abs=1e-6), (
            f"CDS={result.value} != manual={manual_cds}"
        )

    def test_weights_sum_is_used(self, stable_ref, drifted_cur):
        """Total weights used in normalisation should equal sum of enabled descriptor weights."""
        cds = CDSEngine()
        result = cds.compute(stable_ref, drifted_cur)

        expected_total = sum(d.weight for d in DEFAULT_DESCRIPTORS.values() if d.enabled)
        per_desc = result.details["per_descriptor"]
        actual_total = sum(info["weight"] for info in per_desc.values())
        assert actual_total == pytest.approx(expected_total, abs=1e-6)

    def test_each_descriptor_jsd_bounded(self, stable_ref, drifted_cur):
        """Every per-descriptor JSD must be in [0, ln(2)]."""
        cds = CDSEngine()
        result = cds.compute(stable_ref, drifted_cur)
        for name, info in result.details["per_descriptor"].items():
            assert 0 <= info["jsd"] <= np.log(2) + 0.01, (
                f"Descriptor {name}: JSD={info['jsd']} out of bounds"
            )

    def test_empty_descriptors_zero(self):
        """Empty descriptor sets → CDS = 0."""
        cds = CDSEngine()
        result = cds.compute(DescriptorSet(), DescriptorSet())
        assert result.value == 0.0
        assert result.status == "GREEN"

    def test_single_query_per_window(self):
        """
        Minimal case: 1 query in each window.
        JSD of two single-point distributions is degenerate but should not crash.
        """
        ref = DescriptorSet(
            data={
                "response_length": np.array([150.0]),
                "citation_count": np.array([2.0]),
                "refusal_flag": np.array([0.0]),
            },
            query_ids=["ref_0"],
        )
        cur = DescriptorSet(
            data={
                "response_length": np.array([80.0]),
                "citation_count": np.array([0.0]),
                "refusal_flag": np.array([1.0]),
            },
            query_ids=["cur_0"],
        )
        cds = CDSEngine()
        result = cds.compute(ref, cur)
        # Should not crash; value may be 0 due to degenerate histograms
        assert isinstance(result.value, float)


# ═══════════════════════════════════════════════════════════════════════════════
# GROUP 3: PERSISTENCE FILTER
# Mathematical property: requires P consecutive above-threshold readings.
# ═══════════════════════════════════════════════════════════════════════════════

class TestCDSPersistence:
    """
    Persistence filter prevents false alarms from single-window spikes.

    Logic: RED only fires if CDS ≥ amber_threshold for P=3 consecutive calls.
    While CDS may be AMBER in a single window, it won't become RED until
    the condition persists.
    """

    def test_single_spike_not_red(self, stable_ref, drifted_cur):
        """A single elevated CDS should be AMBER, not RED."""
        cds = CDSEngine()
        result = cds.compute(stable_ref, drifted_cur)
        # First window: even if CDS is high, persistence is not met
        assert result.details["persistence_met"] is False

    def test_persistence_triggers_after_p_windows(self, stable_ref, drifted_cur):
        """After P=3 consecutive drifted windows, persistence_met should be True."""
        cds = CDSEngine()
        for i in range(3):
            result = cds.compute(stable_ref, drifted_cur)

        history = result.details["persistence_history"]
        assert len(history) == 3
        assert result.details["persistence_met"] is True

    def test_persistence_resets_when_stable(self, stable_ref, stable_cur, drifted_cur):
        """
        If drift disappears mid-sequence, persistence counter resets.
        [HIGH, HIGH, LOW, HIGH] → persistence_met = False after 4th.
        """
        cds = CDSEngine()
        cds.compute(stable_ref, drifted_cur)  # HIGH
        cds.compute(stable_ref, drifted_cur)  # HIGH
        cds.compute(stable_ref, stable_cur)   # LOW (reset)
        result = cds.compute(stable_ref, drifted_cur)  # HIGH (1st after reset)

        assert result.details["persistence_met"] is False

    def test_persistence_history_bounded(self, stable_ref, drifted_cur):
        """History should not grow unbounded — last 10 entries max."""
        cds = CDSEngine()
        for _ in range(15):
            result = cds.compute(stable_ref, drifted_cur)
        history = result.details["persistence_history"]
        assert len(history) <= 10


# ═══════════════════════════════════════════════════════════════════════════════
# GROUP 4: DESCRIPTOR TYPE HANDLING
# Each of the 3 JSD variants (continuous, discrete, binary) must be
# dispatched correctly based on the descriptor's type field.
# ═══════════════════════════════════════════════════════════════════════════════

class TestDescriptorTypeDispatching:
    """
    Verifies that CDS uses the correct JSD variant for each descriptor type.
    """

    def test_continuous_descriptor_uses_histogram_jsd(self):
        """
        Continuous descriptors (response_length) use histogram-binned JSD.
        Bins create a discrete approximation of the continuous distribution.
        """
        np.random.seed(42)
        ref = np.random.normal(150, 20, 200)
        cur = np.random.normal(100, 20, 200)  # shifted
        jsd = continuous_jsd(ref, cur, bins=50)
        assert jsd > 0.1, "Should detect distribution shift"

    def test_discrete_descriptor_uses_count_jsd(self):
        """
        Discrete descriptors (citation_count) build probability from value counts.
        No binning needed — each unique value gets its own probability.
        """
        ref = np.array([1, 2, 3, 1, 2, 3, 1, 2, 3])
        cur = np.array([0, 0, 0, 1, 1, 1, 0, 0, 0])  # shifted to 0
        jsd = discrete_jsd(ref, cur)
        assert jsd > 0.1

    def test_binary_descriptor_uses_proportion_jsd(self):
        """
        Binary descriptors (refusal_flag) compare two proportions.
        ref_prop=0.1 (10% refusal) vs cur_prop=0.5 (50% refusal).
        """
        jsd = binary_jsd(0.1, 0.5)
        assert jsd > 0.1, "Large shift in binary proportion"

    def test_binary_descriptor_integration(self):
        """
        Test that refusal_flag is treated as binary in the full CDS pipeline.
        Create data with ONLY refusal_flag changing.
        """
        ref = DescriptorSet(
            data={
                "refusal_flag": np.array([0.0] * 90 + [1.0] * 10),  # 10% refusal
            },
            query_ids=[f"r_{i}" for i in range(100)],
        )
        cur = DescriptorSet(
            data={
                "refusal_flag": np.array([0.0] * 50 + [1.0] * 50),  # 50% refusal
            },
            query_ids=[f"c_{i}" for i in range(100)],
        )

        # Use a config with only refusal_flag enabled
        config = CDSConfig(
            descriptors={
                "refusal_flag": CDSDescriptorConfig(weight=1.0, type="binary"),
            }
        )
        cds = CDSEngine(config)
        result = cds.compute(ref, cur)
        assert result.value > 0.1, f"Refusal shift should produce elevated CDS, got {result.value}"


# ═══════════════════════════════════════════════════════════════════════════════
# GROUP 5: THRESHOLD CLASSIFICATION
# ═══════════════════════════════════════════════════════════════════════════════

class TestCDSClassification:
    """
    Classification boundaries:
        GREEN:  CDS < 0.05
        AMBER:  0.05 ≤ CDS < 0.15
        RED:    CDS ≥ 0.15 AND persistence met
    """

    def test_classify_descriptor_green(self):
        """Per-descriptor: JSD < 0.05 → GREEN."""
        cds = CDSEngine()
        assert cds._classify_descriptor(0.01) == "GREEN"
        assert cds._classify_descriptor(0.049) == "GREEN"

    def test_classify_descriptor_amber(self):
        """Per-descriptor: 0.05 ≤ JSD < 0.15 → AMBER."""
        cds = CDSEngine()
        assert cds._classify_descriptor(0.05) == "AMBER"
        assert cds._classify_descriptor(0.14) == "AMBER"

    def test_classify_descriptor_red(self):
        """Per-descriptor: JSD ≥ 0.15 → RED."""
        cds = CDSEngine()
        assert cds._classify_descriptor(0.15) == "RED"
        assert cds._classify_descriptor(0.50) == "RED"

    def test_overall_green(self):
        """Overall: CDS < 0.05 → GREEN regardless of persistence."""
        cds = CDSEngine()
        assert cds._classify_overall(0.02, persistence_met=False) == "GREEN"
        assert cds._classify_overall(0.02, persistence_met=True) == "GREEN"

    def test_overall_amber_without_persistence(self):
        """Overall: 0.05 ≤ CDS < 0.15 without persistence → AMBER."""
        cds = CDSEngine()
        assert cds._classify_overall(0.10, persistence_met=False) == "AMBER"

    def test_overall_amber_high_without_persistence(self):
        """Overall: CDS ≥ 0.15 but no persistence → AMBER (not RED)."""
        cds = CDSEngine()
        assert cds._classify_overall(0.20, persistence_met=False) == "AMBER"

    def test_overall_red_with_persistence(self):
        """Overall: CDS ≥ 0.15 WITH persistence → RED."""
        cds = CDSEngine()
        assert cds._classify_overall(0.20, persistence_met=True) == "RED"


# ═══════════════════════════════════════════════════════════════════════════════
# GROUP 6: EXPLANATION QUALITY
# ═══════════════════════════════════════════════════════════════════════════════

class TestCDSExplanation:
    """Explanations must be actionable — they tell operators WHAT drifted."""

    def test_explanation_contains_cds_value(self, stable_ref, drifted_cur):
        """Explanation must state the CDS value."""
        cds = CDSEngine()
        result = cds.compute(stable_ref, drifted_cur)
        assert "CDS = " in result.explanation

    def test_explanation_lists_top_contributors(self, stable_ref, drifted_cur):
        """Explanation must name the top drift-contributing descriptors."""
        cds = CDSEngine()
        result = cds.compute(stable_ref, drifted_cur)
        assert "Top drift contributors" in result.explanation
        # Should mention at least one descriptor name
        assert any(name in result.explanation for name in DEFAULT_DESCRIPTORS.keys())

    def test_explanation_mentions_persistence_when_met(self, stable_ref, drifted_cur):
        """When persistence triggers, explanation should mention it."""
        cds = CDSEngine()
        for _ in range(3):
            result = cds.compute(stable_ref, drifted_cur)
        if result.details["persistence_met"]:
            assert "persistence" in result.explanation.lower() or "action required" in result.explanation.lower()
